{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camadas de Convolução no PyTorch\n",
    "\n",
    "As camadas de convolução são componentes fundamentais das redes neurais convolucionais (CNNs), amplamente utilizadas para processamento de imagens, análise de vídeo, e aplicações de reconhecimento de padrões. No PyTorch, estas camadas são implementadas através do módulo `torch.nn`, que oferece várias classes para diferentes tipos de convolução.\n",
    "\n",
    "#### Importância nas CNNs\n",
    "\n",
    "As camadas de convolução são essenciais para capturar hierarquias de características visuais — dos detalhes mais simples nos primeiros níveis até padrões complexos e abstratos nas camadas mais profundas. Estas características tornam as CNNs extremamente eficazes para tarefas que dependem da compreensão de contextos visuais e da identificação de objetos em imagens e vídeos.\n",
    "\n",
    "As camadas de convolução no PyTorch são otimizadas para oferecer alto desempenho e flexibilidade, permitindo aos desenvolvedores e pesquisadores construir arquiteturas de CNNs avançadas e eficientes para uma ampla gama de aplicações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sinal\n",
    "x = torch.tensor([5, 4, 8, 7, 9, 3, 6], dtype=torch.float32)\n",
    "# Filtro\n",
    "weight = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "# Tamanho do filtro\n",
    "ks = len(weight)\n",
    "\n",
    "# Redimensiona o sinal para o tamanho 1x1xlen(x). Ou seja, um batch contendo um único sinal, \n",
    "# e esse sinal possui um único canal\n",
    "x = x.reshape(1,1,len(x))\n",
    "# Redimensiona o filtro. O primeiro valor 1 possui um significado diferente do que\n",
    "# no caso do sinal. Depois será explicado.\n",
    "weight = weight.reshape(1,1,len(weight))\n",
    "# Realiza a convolução\n",
    "y = F.conv1d(x, weight)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tamanho da entrada é 7 e da saída é 5. Isso porque o Pytorch realiza a convolução apenas nas posições que não necessitam de preenchimento de borda. Mas modificar o tamanho do resultado é indesejável. \n",
    "\n",
    "É muito comum realizarmos a convolução com padding para manter o tamanho do sinal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "# padding = ks//2 garante que a saída sempre terá o mesmo tamanho que a entrada\n",
    "y = F.conv1d(x, weight, padding=ks//2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado o sinal [5, 4, 8, 7, 9, 3, 6] e filtro [1,2,3], nossa saída deve ser:\n",
    "\n",
    "* y[0] = 1\\*0 + 2\\*5 + 3\\*4 = 22\n",
    "* y[1] = 1\\*5 + 2\\*4 + 3\\*8 = 37\n",
    "* ...\n",
    "* y[6] = 1\\*3 + 2\\*6 + 3\\*0 = 15\n",
    "\n",
    "Note que a função realiza a correlação-cruzada, e não a convolução. Mas para redes neurais isso não importa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[22., 37., 41., 49., 34., 33., 15.]]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais a convolução possui o conceito de bias, que é simplesmente um valor constante que é adicionado ao resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[27., 42., 46., 54., 39., 38., 20.]]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "bias = torch.tensor([5.])\n",
    "# Adiciona o valor 5 a cada elemento do resultado da convolução\n",
    "y_bias = F.conv1d(x, weight, padding=ks//2, bias=bias)\n",
    "print(y_bias)\n",
    "print(torch.allclose(y+bias, y_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada de convolução\n",
    "\n",
    "Uma camada de convolução aplica um conjunto de filtros aprendíveis aos dados de entrada. Cada filtro em uma camada de convolução possui pesos que são aprendidos durante o treinamento do modelo. A operação de convolução envolve a multiplicação desses pesos pelos valores nas regiões da entrada, seguida pela soma dos produtos para produzir um mapa de características (feature map). Este mapa representa informações espaciais que são essenciais para tarefas de visão computacional.\n",
    "\n",
    "#### Tipos de Camadas de Convolução\n",
    "\n",
    "- **`nn.Conv1d`**: Aplicada em dados sequenciais, como séries temporais ou texto (quando os dados são tratados como uma sequência de tokens ou caracteres), operando ao longo de uma única dimensão espacial.\n",
    "\n",
    "- **`nn.Conv2d`**: Esta é a camada de convolução bidimensional mais comumente usada para processar imagens. Ela aplica filtros que deslizam sobre a largura e altura da imagem de entrada para produzir os mapas de características.\n",
    "\n",
    "- **`nn.Conv3d`**: Usada para dados volumétricos ou vídeos, onde a convolução é aplicada em três dimensões — profundidade, altura e largura.\n",
    "\n",
    "#### Parâmetros Comuns em Métodos de Convolução do PyTorch\n",
    "\n",
    "1. **in_channels**:\n",
    "   - **Descrição**: Número de canais na imagem ou tensor de entrada. Por exemplo, imagens RGB têm 3 canais.\n",
    "   - **Tipo**: `int`\n",
    "\n",
    "2. **out_channels**:\n",
    "   - **Descrição**: Número de filtros que a camada de convolução vai aplicar. Este número também determina o número de canais na saída, pois cada filtro produz um mapa de características único.\n",
    "   - **Tipo**: `int`\n",
    "\n",
    "3. **kernel_size**:\n",
    "   - **Descrição**: Tamanho do filtro ou kernel usado na convolução. Pode ser especificado como um único número (para um tamanho de kernel uniforme em todas as dimensões) ou como uma tupla que define o tamanho do kernel para cada dimensão.\n",
    "   - **Tipo**: `int` ou `tuple`\n",
    "\n",
    "4. **stride**:\n",
    "   - **Descrição**: O número de passos que o filtro se move ao longo de cada dimensão da entrada. Um `stride` maior resulta em uma redução proporcional das dimensões do mapa de características de saída. Similar ao `kernel_size`, pode ser um único número ou uma tupla.\n",
    "   - **Tipo**: `int` ou `tuple`\n",
    "\n",
    "5. **padding**:\n",
    "   - **Descrição**: Adiciona bordas de zeros em torno da entrada para permitir que a convolução seja aplicada às bordas da imagem de entrada, mantendo o tamanho da saída. Pode ser especificado como um único número (padding uniforme em todas as bordas) ou como uma tupla para definir padding específico por dimensão.\n",
    "   - **Tipo**: `int` ou `tuple`\n",
    "\n",
    "6. **dilation**:\n",
    "   - **Descrição**: Especifica o espaçamento entre os elementos do kernel. Um `dilation` maior resulta em um \"filtro dilatado\", útil para capturar características em escalas maiores sem aumentar o número de parâmetros. Funciona efetivamente aumentando o campo receptivo do filtro.\n",
    "   - **Tipo**: `int` ou `tuple`\n",
    "\n",
    "7. **groups**:\n",
    "   - **Descrição**: Controla a conexão entre as entradas e as saídas. Por padrão, `groups=1`, o que significa que cada filtro é aplicado a todos os canais de entrada. Se `groups` é igual ao número de canais de entrada, isso resulta numa convolução chamada \"depthwise\". Isso permite que a operação seja separada em grupos de canais, cada um realizando uma convolução independente.\n",
    "   - **Tipo**: `int`\n",
    "\n",
    "8. **bias**:\n",
    "   - **Descrição**: Se `True`, adiciona um termo de viés à saída de cada mapa de características. Normalmente deixado como `True` a menos que a próxima camada imediatamente aplique uma normalização que torne o termo de viés redundante (como batch normalization).\n",
    "   - **Tipo**: `bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1076, -0.2768, -1.5261, -2.0427, -4.5097, -2.2766, -3.0912]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A entrada terá 1 canal, queremos apenas 1 canal de saída. O tamanho do filtro é ks, o \n",
    "# padding é metade do tamanho do filtro e a camada não terá bias\n",
    "conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=ks, padding=ks//2, bias=False)\n",
    "y = conv(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma camada de convolução consiste em um filtro possuindo valores aleatórios. Esse filtro possui o parâmetro requires_grad=True por padrão. Podemos alterar os valores do filtro se quisermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3])\n",
      "Parameter containing:\n",
      "tensor([[[-0.3941, -0.3182,  0.3708]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[22., 37., 41., 49., 34., 33., 15.]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(conv.weight.shape)\n",
    "print(conv.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = weight\n",
    "\n",
    "# Mesmo resultado que a convolução que fizemos antes:\n",
    "conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relação entre convolução e combinação linear\n",
    "\n",
    "Uma convolução nada mais é do que uma combinação linear com menos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[22., 37., 41., 49., 34., 33., 15.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de convolução. Cada linha representa uma posição do kernel. Por exemplo,\n",
    "# a linha 0 fará a operação 2*x[0]+3*x[1]+0*x[2]+...\n",
    "matrix = torch.tensor([[2, 3, 0, 0, 0, 0, 0],\n",
    "                       [1, 2, 3, 0, 0, 0, 0],\n",
    "                       [0, 1, 2, 3, 0, 0, 0],\n",
    "                       [0, 0, 1, 2, 3, 0, 0],\n",
    "                       [0, 0, 0, 1, 2, 3, 0],\n",
    "                       [0, 0, 0, 0, 1, 2, 3],\n",
    "                       [0, 0, 0, 0, 0, 1, 2]], dtype=torch.float32)\n",
    "\n",
    "F.linear(x, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima temos uma matrix 7x7 que recebe 7 atributos de entrada e gera 7 atributos de saída. Mas a combinação linear dos atributos de entrada sempre envolvem apenas 3 parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolução com mais de um canal\n",
    "\n",
    "1. **Entrada da Camada**: \n",
    "   - A entrada para a camada de convolução consiste em um conjunto (batch) de imagens. Por exemplo, se temos um batch com 2 imagens, e cada imagem tem múltiplos canais (como as três cores RGB) e um tamanho específico (altura H e largura W), a entrada é representada por um tensor de quatro dimensões: N (número de imagens no batch) × Cin (número de canais por imagem) × H × W.\n",
    "\n",
    "2. **Filtros da Camada**:\n",
    "   - A camada de convolução utiliza vários filtros (ou kernels) para processar a entrada. Cada filtro é capaz de extrair características específicas da imagem, como bordas, texturas ou outros padrões. Se a camada possui Cout filtros, cada um ajustado para analisar todos os canais de entrada, a estrutura de cada filtro será Cin × KW × KH, onde KW e KH são a largura e altura do filtro, respectivamente.\n",
    "   - Todos os filtros são armazenados em um tensor de tamanho Cout × Cin × KW × KH. Além disso, há um valor de bias para cada filtro, armazenado em um tensor de tamanho Cout.\n",
    "\n",
    "3. **Operação de Convolução**:\n",
    "   - Durante a convolução, cada filtro \"desliza\" sobre a imagem inteira, aplicando-se a cada posição possível da imagem. Em cada posição, o filtro realiza um cálculo que envolve multiplicar seus valores pelos valores correspondentes na imagem e somar todos esses produtos.\n",
    "   - O resultado dessa operação em cada posição é então somado com o valor de bias correspondente ao filtro.\n",
    "\n",
    "4. **Saída da Camada**:\n",
    "   - A saída da camada de convolução é um novo conjunto de imagens (um batch), onde cada imagem agora possui Cout canais, um para cada filtro aplicado. O tamanho de cada imagem de saída (H' × W') depende do tamanho original da imagem, do tamanho do filtro, da quantidade de padding aplicada (adicionando zeros ao redor da imagem para permitir que o filtro se aplique nas bordas) e do stride (quantos pixels o filtro pula após cada aplicação).\n",
    "   - Comumente, o número de canais de saída (Cout) é bastante alto, como 64, 128 ou 256, permitindo que a rede capture uma ampla gama de características.\n",
    "\n",
    "![Camada de Convolução](./conv_layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " tensor([[[0.4016, 0.1977, 0.0868, 0.5421, 0.9579, 0.4345, 0.9196],\n",
      "         [0.3162, 0.1147, 0.3042, 0.3195, 0.3976, 0.4138, 0.6200],\n",
      "         [0.3600, 0.6513, 0.9571, 0.4475, 0.5283, 0.7149, 0.4173],\n",
      "         [0.6390, 0.4250, 0.2405, 0.4718, 0.2524, 0.7026, 0.4903]]])\n",
      "y\n",
      " tensor([[[-0.0387,  0.1202,  0.0074, -0.2349, -0.3178, -0.4426, -0.1302],\n",
      "         [-0.1361, -0.1036, -0.1337, -0.0944, -0.0503, -0.2261,  0.0394],\n",
      "         [ 0.2013, -0.0668, -0.2274, -0.1265,  0.1902,  0.0051, -0.0249],\n",
      "         [ 0.0797,  0.3654,  0.1256,  0.2440,  0.2098,  0.1537,  0.3551],\n",
      "         [ 0.2287,  0.1897,  0.0727,  0.0493,  0.2980,  0.1749,  0.1166]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Batch contendo um sinal de tamanho 7 e com 4 canais\n",
    "x = torch.rand(size=(1,4,7))\n",
    "# Camada que recebe sinal com 4 canais e gera um sinal com 5 canais.\n",
    "conv = nn.Conv1d(in_channels=4, out_channels=5, kernel_size=ks, padding=ks//2, bias=False)\n",
    "\n",
    "y = conv(x)\n",
    "print('x\\n',x)\n",
    "# Saída possui tamanho 1x5x7\n",
    "print('y\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O **número de canais de saída** define o **número de filtros** que serão utilizados na camada de convolução. No nosso caso, temos 5 filtros\n",
    "* Cada filtro possui **tamanho espacial ks**\n",
    "* Cada filtro possui **profundidade 4**, pois o sinal de entrada possui 4 canais.\n",
    "* Portanto, temos 5 filtros de tamanho 4 x ks cada\n",
    "* Portanto, o tamanho do tensor .weight da camada de convolução possui tamanho 5 x 4 x ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Filtro 1 da camada\n",
    "filtro1 = conv.weight[0]\n",
    "# Região do sinal que corresponde quando o filtro está na posição 1\n",
    "regiao = x[0,:,0:3]\n",
    "# Resultado da convolução para esse ponto específico\n",
    "res = (filtro1*regiao).sum()\n",
    "# comparação do resultado\n",
    "print(torch.allclose(y[0,0,1], res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros parâmetros da convolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride\n",
    "\n",
    "O stride refere-se ao `número de pixels que o filtro se desloca` ao longo das dimensões de entrada após cada operação de convolução. Em termos simples, o stride controla o quão longe o filtro se move cada vez que é aplicado durante a convolução. Um stride maior resulta em uma redução das dimensões do mapa de características produzido, já que o filtro é aplicado menos vezes sobre a entrada. Por exemplo, um stride de 2 significa que o filtro pula um pixel (ou unidade de medida em outras dimensões de dados) para cada passo, reduzindo aproximadamente pela metade o tamanho do mapa de características em comparação com um stride de 1, assumindo outros parâmetros constantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[37., 49., 33.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5, 4, 8, 7, 9, 3, 6], dtype=torch.float32)\n",
    "weight = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "x = x.reshape(1,1,len(x))\n",
    "weight = weight.reshape(1,1,len(weight))\n",
    "\n",
    "# Faz o filtro deslocar duas posições ao invés de deslocar uma posição por vez\n",
    "y = F.conv1d(x, weight, stride=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# O resultado de stride=2 é equivalente a indexar a saída de stride=1 pulando 2 índices\n",
    "yt = F.conv1d(x, weight, stride=1)\n",
    "print(torch.allclose(y, yt[0,0,::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilatação\n",
    "\n",
    "A dilatação, por sua vez, envolve a `inserção de espaços entre os pixels ou unidades dentro do filtro` usado na operação de convolução. Este processo aumenta efetivamente o tamanho ou a área do filtro sem adicionar novos pesos ao modelo. A dilatação permite que o filtro abranja uma área maior da entrada com o mesmo número de parâmetros, aumentando o campo receptivo do filtro. Por exemplo, uma dilatação de 2 significa que há um espaço de um pixel entre cada unidade do filtro, tornando o filtro espacialmente maior. Isso é especialmente útil para capturar informações em escalas maiores sem aumentar a complexidade computacional do modelo, já que o número de parâmetros permanece o mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[48., 27., 44.]]])\n"
     ]
    }
   ],
   "source": [
    "y = F.conv1d(x, weight, dilation=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# dilatação=2 é equivalente a inserir 0 entre os valores do filtro\n",
    "wt = torch.tensor([1, 0, 2, 0, 3], dtype=torch.float32).reshape(1,1,-1)\n",
    "yt = F.conv1d(x, wt, dilation=1)\n",
    "print(torch.allclose(y, yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impacto do Stride e da Dilatação\n",
    "\n",
    "O uso de stride e dilatação afeta diretamente a forma como as camadas de convolução percebem e processam as entradas. Um stride maior pode acelerar o processamento e reduzir a resolução espacial dos mapas de características, útil para aumentar a abstração e reduzir o consumo de memória em redes profundas. A dilatação, por outro lado, permite que o modelo capture contextos mais amplos ou padrões de maior escala sem perder a resolução espacial ou aumentar o número de parâmetros, mantendo a eficiência computacional. Ambos os parâmetros são ajustáveis no PyTorch e podem ser configurados de acordo com as necessidades específicas da tarefa de aprendizado de máquina sendo abordada.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
